# -*- coding: utf-8 -*-
"""MINISOLUTIONCODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kxq8XhhYnb9zE4NUMBrU5CEtTBi2h-Ni
"""

# Commented out IPython magic to ensure Python compatibility.
from dataclasses import dataclass
import pandas as pd
#pandas-> Data Analysis
import numpy as np
#mathematical operations on arrays
import matplotlib.pyplot as plt
#plots
# %matplotlib inline
import seaborn as sns
#Works easily with the df and pds
from datetime import date, datetime
from sklearn.preprocessing import MinMaxScaler
#for preprocessing

# Columns include open_date, city, location, P1-P37->(demographic data, realestate and commercial dataclass)



train_df = pd.read_csv('train.csv',parse_dates=['Open Date'])
test_df = pd.read_csv('test.csv',parse_dates=['Open Date'])

train_df.shape, test_df.shape

train_df.describe()
#mean, s.d., count, max, min
test_df.describe()

train_df.isnull().sum()

test_df.isnull().sum()

def print_cols():
    print(train_df.columns)

plt.figure(figsize=(20,20))
sns.heatmap(train_df.corr(),annot=True, cbar=False, cmap='Reds')
#get summmary about the data

print_cols()

plt.figure(figsize=(20,4))
sns.boxplot(x='revenue',data=train_df)#outliers
#-->
from scipy.stats import iqr

upper_limit = train_df.revenue.quantile(0.75) + (1.5* iqr(train_df.revenue))
lower_limit = train_df.revenue.quantile(0.25)- (1.5* iqr(train_df.revenue))

condition = (train_df.revenue > upper_limit) | (train_df.revenue<lower_limit)
train_df[condition]

rev_filter = (train_df.revenue < 10000000)
train_df = train_df[rev_filter]

train_df.shape
#--^
#To get characteristics of the data ---> EDA(Exploratory data analysis)
len(train_df.Id.unique()) == train_df.shape[0]

len(test_df.Id.unique()) == test_df.shape[0]

train_df.drop('Id',axis=1,inplace=True)
test_df.drop('Id', axis=1, inplace=True)

train_df.shape, test_df.shape

train_df['Open Date'].value_counts()

train_df['open_year'] = train_df['Open Date'].dt.year
# Do it to the test data 
test_df['open_year'] = test_df['Open Date'].dt.year

train_df['City Group'].value_counts()
#visualizing data acc. with open yr
fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4))
train_df.groupby(['City Group']).mean()['revenue'].plot.bar(ax=ax1)
sns.boxplot(x='City Group',y='revenue', data=train_df,ax=ax2)

train_df.groupby(['City']).mean()['revenue'].plot.barh(figsize=(20,20))
plt.yticks(fontsize=17)

len(train_df.City.unique()) ,len(test_df.City.unique())

train_df.drop('City',axis=1,inplace=True)
test_df.drop('City',axis=1,inplace=True)

print_cols()

train_df.Type.value_counts()

train_df.loc[124,'Type'] = 'IL'

test_df.Type.value_counts()

fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4))
train_df.groupby(['Type']).mean()['revenue'].plot.bar(ax=ax1)
sns.boxplot(train_df.Type, train_df.revenue,ax=ax2)
plt.show()

train_df.P1.value_counts()

train_df.head()

test_df.head()

comp_df = pd.concat([train_df, test_df])
comp_df.reset_index(drop=True, inplace=True)

comp_df.Type.value_counts(),comp_df['City Group'].value_counts()

comp_df.Type = comp_df.Type.map({'MB':0,'DT':1, 'IL':2,'FC':3})
comp_df['City Group'] = comp_df['City Group'].map({'Big Cities':1, 'Other':0})

comp_df.head(3)
#Normzalization using minmaxscaler
p_name = ['P'+str(i) for i in range(1,38)]
comp_df[p_name] = MinMaxScaler().fit_transform(comp_df[p_name])


#Principal component Analysis
from sklearn.decomposition import PCA
pca = PCA().fit(comp_df[p_name])
plt.figure(figsize=(7,5))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of Components')
plt.ylabel('Explained variance')
plt.yticks(np.arange(0.1,1.1,0.05))
plt.xticks(np.arange(0,41,2))
plt.grid(True)

pca_list = ['pca'+str(i) for i in range(1,30,1)]
comp_df[pca_list] = PCA(n_components=29).fit_transform(comp_df[p_name])
comp_df.drop(p_name,axis=1,inplace=True)

comp_df

import datetime
comp_df['launch_days'] = (datetime.datetime.now() - comp_df[['Open Date']])
comp_df['launch_days'] = comp_df['launch_days'].dt.days

comp_df.drop('Open Date',axis=1,inplace=True)

comp_df['launch_days'] = MinMaxScaler().fit_transform(comp_df[['launch_days']])

test_df = comp_df[comp_df['revenue'].isnull()]
train_df = comp_df[comp_df['revenue'].notnull()]
test_df.drop('revenue',axis=1, inplace=True)

train_df.shape, test_df.shape

x_train = train_df.drop('revenue',axis=1)
y_train = train_df['revenue']

#LightGBM is a gradient boosting framework that uses tree based learning algorithms
# Faster training speed and higher efficiency.

# Lower memory usage.

# Better accuracy.

# Support of parallel, distributed, and GPU learning.

# Capable of handling large-scale data.
from lightgbm import LGBMRegressor
from sklearn.model_selection import GridSearchCV, cross_validate, RepeatedKFold

cv = RepeatedKFold(n_splits=10, n_repeats=3)
scores = cross_validate(LGBMRegressor(), x_train,y_train, scoring=['r2','neg_root_mean_squared_error'],cv=cv)

r2 = scores['test_r2']
rmse = scores['test_neg_root_mean_squared_error']
print(np.mean(r2),np.mean(rmse))

final_model = LGBMRegressor(boosting_type='dart',max_depth=3,n_estimators=20,random_state=42, subsample=0.3).fit(x_train,y_train)

test_file = pd.read_csv('test.csv')
answer = pd.DataFrame(final_model.predict(test_df))
answer.columns = ['Prediction']
answer['Id'] = test_file.index.tolist()
answer.set_index('Id',inplace=True)

answer.to_csv('revenue.csv')

